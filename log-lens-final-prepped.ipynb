{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c528267",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 LogLens: GenAI-Powered Root Cause Analysis Assistant for Logs\n",
    "\n",
    "## 🎯 Use Case\n",
    "\n",
    "In modern distributed systems, debugging failures across Spark, Kafka, and Airflow can be extremely time-consuming. Engineers often search logs, check documentation, and look for community solutions to resolve recurring issues. **LogLens** transforms this experience by acting as a **GenAI-powered RCA (Root Cause Analysis) assistant**, reducing resolution time and offering clear, actionable suggestions with high confidence.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ GenAI Capabilities Used\n",
    "\n",
    "This notebook integrates **three or more GenAI capabilities**:\n",
    "\n",
    "1. **Retrieval-Augmented Generation (RAG)** using `ChromaDB` for retrieving similar historical logs.\n",
    "2. **LLM Reasoning** using `Gemini 2.0 Flash` to generate human-readable error explanations and root causes.\n",
    "3. **Web Search + Summarization** via `SerpAPI` + `Gemini` to enhance LLM output with live online content.\n",
    "4. **Confidence Scoring and Audit Agent**: Comparing Gemini vs external answers to improve trust and quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Objective\n",
    "\n",
    "- Simplify debugging for distributed systems.\n",
    "- Provide clean, human-readable fixes with supporting explanations.\n",
    "- Automate log analysis using advanced GenAI flows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8e5b5",
   "metadata": {},
   "source": [
    "\n",
    "## 🔄 Solution Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[User Inputs Error Log] --> B[Retrieve Similar Logs from ChromaDB]\n",
    "    B --> C[Gemini Agent: Generate RCA]\n",
    "    C --> D[Web Agent: Search StackOverflow/GitHub]\n",
    "    D --> E[Gemini: Summarize External Solutions]\n",
    "    C --> F[Audit Agent: Compare Gemini vs Web Fix]\n",
    "    F --> G[Return Final Fix with Confidence Score]\n",
    "    G --> H[Suggest Follow-Up Questions & Learning Resources]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62023c9b",
   "metadata": {},
   "source": [
    "\n",
    "## 🔍 Step-by-Step Walkthrough\n",
    "\n",
    "1. **Log Simulation & Storage**:\n",
    "    - Simulated ~30+ logs across Spark, Kafka, and Airflow.\n",
    "    - Stored them in ChromaDB with embedded vectors.\n",
    "\n",
    "2. **User Error Input**:\n",
    "    - User enters an error like: `org.apache.spark.shuffle.FetchFailedException...`\n",
    "\n",
    "3. **LLM RCA Agent**:\n",
    "    - Gemini retrieves similar logs and summarizes the root cause + fixes in plain English.\n",
    "\n",
    "4. **Web Fix Agent**:\n",
    "    - Uses SerpAPI to find relevant StackOverflow or GitHub results.\n",
    "    - Gemini summarizes external findings into readable fixes.\n",
    "\n",
    "5. **Audit Agent**:\n",
    "    - Compares Gemini RCA vs Web fix, then gives a confidence score and verdict.\n",
    "\n",
    "6. **Chatbot Interface**:\n",
    "    - Keeps the user engaged with follow-up prompts and allows continuous queries.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Output Format Example\n",
    "\n",
    "```\n",
    "Spark failed to fetch shuffle data. This usually happens when workers crash or run out of memory.\n",
    "Fix: Try increasing memory, disabling dynamic executor removal, and reviewing executor logs.\n",
    "Confidence: 0.85\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Learning Recommendations\n",
    "\n",
    "For each error, the assistant provides:\n",
    "- Key Spark/Kafka/Airflow documentation links\n",
    "- Concepts like dynamic allocation, GC tuning, etc.\n",
    "- Tools like Spark UI or monitoring systems\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Limitations & Future Work\n",
    "\n",
    "- Currently works on simulated logs; can be extended to live ingestion from production systems.\n",
    "- Search depends on SerpAPI rate limits.\n",
    "- Confidence score is heuristic; can be improved using LLM evaluation frameworks.\n",
    "- Future plans: Streamlit UI, Slack bot integration, CI/CD DevOps hooks.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Conclusion\n",
    "\n",
    "LogLens is a GenAI-first RCA assistant that saves time, boosts engineering productivity, and makes debugging accessible for all engineers. The combination of RAG, LLMs, and web scraping provides a powerful triage workflow.\n",
    "\n",
    "👉 Try entering your own error at the bottom of this notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-16T23:58:39.925479Z",
     "iopub.status.busy": "2025-04-16T23:58:39.925146Z",
     "iopub.status.idle": "2025-04-16T23:58:41.711149Z",
     "shell.execute_reply": "2025-04-16T23:58:41.710288Z",
     "shell.execute_reply.started": "2025-04-16T23:58:39.925455Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T23:58:56.968580Z",
     "iopub.status.busy": "2025-04-16T23:58:56.968137Z",
     "iopub.status.idle": "2025-04-16T23:59:45.156383Z",
     "shell.execute_reply": "2025-04-16T23:59:45.155441Z",
     "shell.execute_reply.started": "2025-04-16T23:58:56.968552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip uninstall -y google google-cloud-aiplatform google-genai -q\n",
    "!pip install -q google-generativeai chromadb serpapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T23:59:53.655619Z",
     "iopub.status.busy": "2025-04-16T23:59:53.655280Z",
     "iopub.status.idle": "2025-04-16T23:59:58.517389Z",
     "shell.execute_reply": "2025-04-16T23:59:58.516507Z",
     "shell.execute_reply.started": "2025-04-16T23:59:53.655589Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import random\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Get API keys from Kaggle secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "genai.configure(api_key=user_secrets.get_secret(\"google_api_key\"))\n",
    "serpapi_key = user_secrets.get_secret(\"serpapi_key\")\n",
    "\n",
    "# Load Gemini model\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Fake Logs + Store in ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T00:00:34.371102Z",
     "iopub.status.busy": "2025-04-17T00:00:34.369700Z",
     "iopub.status.idle": "2025-04-17T00:00:51.645204Z",
     "shell.execute_reply": "2025-04-17T00:00:51.644273Z",
     "shell.execute_reply.started": "2025-04-17T00:00:34.371060Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 38.1MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample log errors for simulation\n",
    "error_catalog = {\n",
    "    \"Spark\": [\n",
    "        (\"OutOfMemoryError\", \"Job aborted: java.lang.OutOfMemoryError: Java heap space\", \"Increase executor memory\"),\n",
    "        (\"NotSerializableException\", \"Task not serializable: java.io.NotSerializableException\", \"Make UDF class Serializable\"),\n",
    "        (\"ClassNotFoundException\", \"Caused by: java.lang.ClassNotFoundException\", \"Add missing JAR dependency\"),\n",
    "        (\"NullPointerException\", \"Exception: java.lang.NullPointerException\", \"Add null checks in code\"),\n",
    "        (\"AnalysisException\", \"cannot resolve column in input schema\", \"Fix column name or select statement\"),\n",
    "        (\"DiskFullError\", \"Executor lost: No space left on device\", \"Clean up disk or use bigger disk\"),\n",
    "        (\"StageRetryLimit\", \"Stage failed 4 times, aborting job\", \"Fix data skew or memory issues\"),\n",
    "        (\"FetchFailedException\", \"org.apache.spark.shuffle.FetchFailedException\", \"Investigate shuffle configuration\"),\n",
    "        (\"FileNotFoundException\", \"Input path does not exist: s3://...\", \"Verify file path in job config\"),\n",
    "        (\"SparkSubmitError\", \"spark-submit failed with exit code 1\", \"Validate spark-submit command and configs\")\n",
    "    ],\n",
    "    \"Airflow\": [\n",
    "        (\"BrokenDAG\", \"Broken DAG: No module named 'plugin'\", \"Ensure plugin exists in airflow/plugins\"),\n",
    "        (\"TriggerRuleError\", \"Invalid trigger rule: ALL_WRONG\", \"Use valid trigger like all_success\"),\n",
    "        (\"FileNotFoundError\", \"FileNotFoundError: 'data.csv' not found\", \"Check file path or upstream task\"),\n",
    "        (\"TaskSkipped\", \"Task skipped due to dependency\", \"Ensure upstream tasks are healthy\"),\n",
    "        (\"NoneTypeError\", \"'NoneType' object has no attribute 'write'\", \"Initialize object before usage\"),\n",
    "        (\"TaskTimeout\", \"Task timed out after 300s\", \"Increase timeout or optimize task\"),\n",
    "        (\"ImportError\", \"ImportError: cannot import airflow.providers...\", \"Check module install or DAG syntax\"),\n",
    "        (\"InvalidCron\", \"Invalid cron expression: */99 * * * *\", \"Fix cron syntax\"),\n",
    "        (\"SQLAlchemyError\", \"sqlalchemy.exc.OperationalError\", \"Check DB connectivity and credentials\"),\n",
    "        (\"DeadlockError\", \"Scheduler deadlock: no heartbeat from workers\", \"Scale out workers or debug DAGs\")\n",
    "    ],\n",
    "    \"Kafka\": [\n",
    "        (\"ConsumerLag\", \"[WARN] Consumer lag high: 50000\", \"Scale up consumer instances\"),\n",
    "        (\"SSLHandshakeError\", \"SSL handshake failed with broker\", \"Check SSL cert and config\"),\n",
    "        (\"TopicNotFound\", \"No such topic 'user_events'\", \"Create topic before consuming\"),\n",
    "        (\"KafkaTimeout\", \"Timeout expired while committing offsets\", \"Check broker latency or partition load\"),\n",
    "        (\"StuckConsumer\", \"Consumer stuck for 10+ mins\", \"Restart or debug consumer group\"),\n",
    "        (\"OffsetOutOfRange\", \"OffsetOutOfRangeException\", \"Reset offset to earliest/latest\"),\n",
    "        (\"LeaderNotAvailable\", \"No leader for partition 0\", \"Restart broker or check cluster state\"),\n",
    "        (\"BufferExhaustedException\", \"Buffer full, producer failed\", \"Increase buffer or reduce message rate\"),\n",
    "        (\"UnknownTopicOrPartition\", \"Unknown topic or partition\", \"Check spelling and Kafka setup\"),\n",
    "        (\"RebalanceInProgress\", \"RebalanceInProgressException\", \"Wait or tune rebalance configs\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate logs\n",
    "def generate_logs():\n",
    "    logs = []\n",
    "    now = datetime.utcnow()\n",
    "    for system, errors in error_catalog.items():\n",
    "        for idx, (etype, msg, fix) in enumerate(errors):\n",
    "            logs.append({\n",
    "                \"log_id\": f\"{system.lower()}-{idx:03}\",\n",
    "                \"component\": system,\n",
    "                \"timestamp\": (now - timedelta(minutes=random.randint(1, 5000))).isoformat() + \"Z\",\n",
    "                \"error_type\": etype,\n",
    "                \"content\": msg,\n",
    "                \"expected_fix\": fix,\n",
    "                \"is_resolved\": False\n",
    "            })\n",
    "    return logs\n",
    "\n",
    "logs = generate_logs()\n",
    "\n",
    "# Store in ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"/kaggle/working/chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(\"logs\")\n",
    "\n",
    "for log in logs:\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    text = f\"{log['component']} | {log['error_type']}: {log['content']}\"\n",
    "    collection.add(\n",
    "        ids=[doc_id],\n",
    "        documents=[text],\n",
    "        metadatas=[{\n",
    "            \"log_id\": log[\"log_id\"],\n",
    "            \"component\": log[\"component\"],\n",
    "            \"error_type\": log[\"error_type\"],\n",
    "            \"expected_fix\": log[\"expected_fix\"]\n",
    "        }]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similar Log Retrieval + RCA Agent (Text Response)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T00:11:09.642999Z",
     "iopub.status.busy": "2025-04-17T00:11:09.642579Z",
     "iopub.status.idle": "2025-04-17T00:11:09.650779Z",
     "shell.execute_reply": "2025-04-17T00:11:09.649584Z",
     "shell.execute_reply.started": "2025-04-17T00:11:09.642973Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_similar_logs(query_text, top_k=3):\n",
    "    results = collection.query(query_texts=[query_text], n_results=top_k)\n",
    "    return results[\"documents\"][0], results[\"metadatas\"][0]\n",
    "\n",
    "def gemini_rca_summary(log_entry):\n",
    "    docs, metas = retrieve_similar_logs(log_entry[\"content\"])\n",
    "    context = \"\\n\".join(\n",
    "        f\"- Log: {doc}\\n  Fix: {meta['expected_fix']}\"\n",
    "        for doc, meta in zip(docs, metas)\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You're a helpful assistant for debugging system logs.\n",
    "\n",
    "Here are similar logs:\n",
    "{context}\n",
    "\n",
    "Now, for the following issue:\n",
    "\"{log_entry['content']}\"\n",
    "\n",
    "Give a very short and simple explanation:\n",
    "1. What the issue is (in 1–2 lines)\n",
    "2. What caused it (in simple words)\n",
    "3. What can fix it (2–3 quick suggestions)\n",
    "4. End with a confidence score between 0 and 1\n",
    "\n",
    "Be brief. Use everyday language. No jargon. No code. No JSON. Just clear plain text.\n",
    "\"\"\"\n",
    "\n",
    "    return model.generate_content(prompt).text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Web Search + Summarizer Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T00:11:12.032628Z",
     "iopub.status.busy": "2025-04-17T00:11:12.032254Z",
     "iopub.status.idle": "2025-04-17T00:11:12.040170Z",
     "shell.execute_reply": "2025-04-17T00:11:12.038506Z",
     "shell.execute_reply.started": "2025-04-17T00:11:12.032604Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_and_summarize_web(log_text):\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": f\"{log_text} site:stackoverflow.com OR site:github.com\",\n",
    "        \"api_key\": serpapi_key,\n",
    "        \"num\": \"5\"\n",
    "    }\n",
    "    res = requests.get(\"https://serpapi.com/search\", params=params).json()\n",
    "    snippets = [r.get(\"snippet\", \"\") for r in res.get(\"organic_results\", [])][:3]\n",
    "    \n",
    "    context = \"\\n\".join(snippets)\n",
    "    \n",
    "    if not context:\n",
    "        return \"No relevant info found online.\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a GenAI assistant. Given these web snippets:\n",
    "\n",
    "{context}\n",
    "\n",
    "Summarize the most effective fix or strategy for this issue.\n",
    "\"\"\"\n",
    "    return model.generate_content(prompt).text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare LLM vs Web – Audit Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T00:12:22.137349Z",
     "iopub.status.busy": "2025-04-17T00:12:22.136988Z",
     "iopub.status.idle": "2025-04-17T00:12:22.143931Z",
     "shell.execute_reply": "2025-04-17T00:12:22.142714Z",
     "shell.execute_reply.started": "2025-04-17T00:12:22.137326Z"
    }
   },
   "outputs": [],
   "source": [
    "def audit_agent(log_text, gemini_fix, web_fix):\n",
    "    prompt = f\"\"\"\n",
    "You are an audit agent comparing two solutions:\n",
    "\n",
    "Log: {log_text}\n",
    "\n",
    "--- Gemini's RCA ---\n",
    "{gemini_fix}\n",
    "\n",
    "--- External Web Fix ---\n",
    "{web_fix}\n",
    "\n",
    "Evaluate which is more accurate, what's missing, and give a confidence score (0–1).\n",
    "Return plain text (no JSON).\n",
    "\"\"\"\n",
    "    return model.generate_content(prompt).text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T00:12:23.717701Z",
     "iopub.status.busy": "2025-04-17T00:12:23.717366Z",
     "iopub.status.idle": "2025-04-17T00:12:23.725251Z",
     "shell.execute_reply": "2025-04-17T00:12:23.724377Z",
     "shell.execute_reply.started": "2025-04-17T00:12:23.717676Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_rca_chat():\n",
    "    print(\"👋 Welcome to LogLens AI Assistant!\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter the error you faced (or type 'exit' to quit):\\n> \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"👋 Goodbye! Stay bug-free.\")\n",
    "            break\n",
    "        \n",
    "        # Create log structure\n",
    "        log = {\n",
    "            \"log_id\": \"user-log\",\n",
    "            \"component\": \"Unknown\",\n",
    "            \"error_type\": \"UserInput\",\n",
    "            \"content\": user_input,\n",
    "            \"is_resolved\": False,\n",
    "            \"expected_fix\": None\n",
    "        }\n",
    "\n",
    "        print(\"\\n🤖 Analyzing with Gemini + ChromaDB...\")\n",
    "        gemini_response = gemini_rca_summary(log)\n",
    "        print(\"\\n🧠 Gemini RCA Suggestion:\")\n",
    "        print(gemini_response)\n",
    "\n",
    "        print(\"\\n🔍 Searching web for external solutions...\")\n",
    "        web_fix = search_and_summarize_web(user_input)\n",
    "        print(\"\\n🌐 Web-Based Fix Summary:\")\n",
    "        print(web_fix)\n",
    "\n",
    "        print(\"\\n📊 Comparing both responses...\")\n",
    "        audit_result = audit_agent(user_input, gemini_response, web_fix)\n",
    "        print(\"\\n📢 Final Verdict:\")\n",
    "        print(audit_result)\n",
    "\n",
    "        print(\"\\n💡 You can now enter a follow-up question or another error.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T00:12:27.205629Z",
     "iopub.status.busy": "2025-04-17T00:12:27.205304Z",
     "iopub.status.idle": "2025-04-17T00:15:08.468464Z",
     "shell.execute_reply": "2025-04-17T00:15:08.467281Z",
     "shell.execute_reply.started": "2025-04-17T00:12:27.205607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Welcome to LogLens AI Assistant!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the error you faced (or type 'exit' to quit):\n",
      ">  Lost executor 1 on host: Executor heartbeat timed out after 128083 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Analyzing with Gemini + ChromaDB...\n",
      "\n",
      "🧠 Gemini RCA Suggestion:\n",
      "Log: Spark | ExecutorTimeout: Executor heartbeat timed out\n",
      "\n",
      "1. Issue: A Spark executor stopped talking to the main process.\n",
      "2. Cause: The executor took too long to respond, probably due to being overloaded or slow network.\n",
      "3. Fix: Give executors more resources, check network, or increase timeout.\n",
      "4. Confidence: 0.8\n",
      "\n",
      "🔍 Searching web for external solutions...\n",
      "\n",
      "🌐 Web-Based Fix Summary:\n",
      "No relevant info found online.\n",
      "\n",
      "📊 Comparing both responses...\n",
      "\n",
      "📢 Final Verdict:\n",
      "Gemini's RCA is a reasonable and generally correct assessment. The log message clearly indicates an executor timeout, suggesting the executor became unresponsive within the specified time limit (128083 ms). The proposed causes (overloaded executor, slow network) are the most likely culprits. The suggested fixes (more resources, network check, increased timeout) are also standard troubleshooting steps.\n",
      "\n",
      "What's missing:\n",
      "\n",
      "*   More specific debugging steps. While the suggested fixes are good, they lack detail. For example, \"check network\" is vague. It should include specific network checks (latency, packet loss, bandwidth). Similarly, increasing the timeout is a workaround, not a fix, and might mask an underlying problem. The RCA doesn't mention potential code bottlenecks within the executor.\n",
      "*   Consideration of garbage collection pauses: Long GC pauses are a frequent cause of executor timeouts in Spark and should be considered in the RCA.\n",
      "*   Monitoring recommendations: The RCA doesn't suggest any monitoring strategies to proactively identify and prevent future executor timeouts.\n",
      "\n",
      "Comparison:\n",
      "\n",
      "Since the external web fix provided no relevant info, Gemini's RCA is the better answer.\n",
      "\n",
      "Confidence: 0.75 (slightly lowered from 0.8 because it lacks specific debugging steps and consideration of GC pauses)\n",
      "\n",
      "💡 You can now enter a follow-up question or another error.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the error you faced (or type 'exit' to quit):\n",
      ">  spark.conf.set(\"spark.task.maxFailures\", \"4\") wouldnt this help?Is this what you suggested in the previous response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Analyzing with Gemini + ChromaDB...\n",
      "\n",
      "🧠 Gemini RCA Suggestion:\n",
      "1. It looks like you're trying to increase the number of times a task can fail before the whole Spark job fails.\n",
      "2. This usually happens when tasks fail due to flaky data or temporary issues.\n",
      "3. Setting `spark.task.maxFailures` can help, but it's a band-aid. Dig into *why* the tasks are failing in the first place. Check for bad data or resource problems.\n",
      "4. Confidence: 0.6\n",
      "\n",
      "🔍 Searching web for external solutions...\n",
      "\n",
      "🌐 Web-Based Fix Summary:\n",
      "The provided snippets touch on issues related to Apache Spark job failures, specifically:\n",
      "\n",
      "*   A job being killed with a \"SparkDeploySchedulerBackend Error: Application has been killed\" message.\n",
      "*   The need to set timeouts for Spark tasks or map operations to skip long-running tasks.\n",
      "*   Limiting the number of retries on Spark job failure.\n",
      "\n",
      "While these snippets don't offer a single \"most effective fix\", they highlight strategies to *mitigate* issues that lead to job failures and improve resilience:\n",
      "\n",
      "1.  **Address the Root Cause of \"Application has been killed\":** The error \"SparkDeploySchedulerBackend Error: Application has been killed\" indicates a more fundamental problem. This could be due to resource exhaustion (insufficient memory, CPU), exceeding cluster limits, or an external process killing the application. The most effective approach here is to investigate the logs and identify *why* the application was killed and address that underlying issue.\n",
      "\n",
      "2.  **Implement Timeouts:** Setting timeouts for tasks can prevent a single long-running task from blocking the entire job. If a task exceeds the timeout, it can be skipped or retried.\n",
      "\n",
      "3.  **Limit Retries:** Limiting the number of retries prevents the job from endlessly retrying a failing task, which can consume resources and delay completion.  This prevents the job from getting stuck in a loop of failed attempts.\n",
      "\n",
      "Therefore, there isn't one single \"fix\". It's a multi-faceted approach: **diagnose the reason for the job failure first**, then employ timeouts and retry limits as preventative measures to handle transient issues or problematic tasks.\n",
      "\n",
      "📊 Comparing both responses...\n",
      "\n",
      "📢 Final Verdict:\n",
      "Here's an evaluation of the two responses:\n",
      "\n",
      "**Accuracy:**\n",
      "\n",
      "*   **Gemini's RCA:** Correctly identifies the purpose of `spark.task.maxFailures`. It also correctly advises that this setting is a workaround and not a root cause solution. The advice to investigate the underlying cause of task failures (bad data, resource problems) is good.\n",
      "*   **External Web Fix:** This response is more comprehensive and accurate. It acknowledges that increasing `spark.task.maxFailures` is not the primary solution and emphasizes identifying the \"Application has been killed\" cause. The suggestions of timeouts and limiting retries are valuable strategies. It correctly frames the issue as requiring diagnosis first, followed by preventative measures.\n",
      "\n",
      "**What's Missing:**\n",
      "\n",
      "*   **Gemini's RCA:** While it mentions bad data and resource problems, it could be more specific about common resource issues (memory, CPU, disk space). It also doesn't address the specific error message mentioned (\"Application has been killed\").\n",
      "*   **External Web Fix:** It could benefit from briefly suggesting specific tools or logs to investigate the \"Application has been killed\" error.\n",
      "\n",
      "**Comparison and Confidence:**\n",
      "\n",
      "The \"External Web Fix\" is more accurate and helpful because it focuses on diagnosing the underlying problems rather than just suggesting a workaround. It also provides a more complete picture of strategies for dealing with Spark job failures. While Gemini's response is not wrong, it lacks the depth and actionable advice of the web fix.\n",
      "\n",
      "**Confidence Score:**\n",
      "\n",
      "*   **Gemini's RCA:** 0.7\n",
      "*   **External Web Fix:** 0.9\n",
      "\n",
      "💡 You can now enter a follow-up question or another error.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the error you faced (or type 'exit' to quit):\n",
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Goodbye! Stay bug-free.\n"
     ]
    }
   ],
   "source": [
    "run_rca_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
